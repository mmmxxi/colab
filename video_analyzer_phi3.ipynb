{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§  Video Analyzer with Phi-3 Vision & Whisper\n",
        "Uploads a video â†’ extracts frames â†’ describes them using **Phi-3-vision** â†’ transcribes audio with **Whisper** â†’ generates a final report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages with version pinning\n",
        "!pip install -q numpy<2 transformers==4.41.0 accelerate==0.29.3 pillow==10.3.0 torch==2.3.1 \\\n",
        "  torchvision==0.18.1 opencv-python==4.9.0.80 moviepy==1.0.3 whisper==1.1.10 \\\n",
        "  ffmpeg-python==0.2.0 timm==0.9.16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Upload video with validation\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"Upload your video (.mp4, .mov, etc.)\")\n",
        "uploaded = files.upload()\n",
        "video_path = list(uploaded.keys())[0]\n",
        "\n",
        "if not os.path.exists(video_path):\n",
        "    raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
        "    \n",
        "print(f\"Uploaded video ({os.path.getsize(video_path)/1024/1024:.2f} MB): {video_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Robust frame extraction with error handling\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "output_folder = 'frames'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Clear existing frames\n",
        "for f in os.listdir(output_folder):\n",
        "    os.remove(os.path.join(output_folder, f))\n",
        "\n",
        "vidcap = cv2.VideoCapture(video_path)\n",
        "if not vidcap.isOpened():\n",
        "    raise RuntimeError(\"Error opening video file\")\n",
        "\n",
        "fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "if fps <= 0:\n",
        "    fps = 30  # Default fallback\n",
        "\n",
        "frame_interval = int(fps * 2)  # Capture every 2 seconds\n",
        "saved_count = 0\n",
        "total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "for frame_num in range(0, total_frames, frame_interval):\n",
        "    vidcap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "    success, image = vidcap.read()\n",
        "    \n",
        "    if success:\n",
        "        cv2.imwrite(f\"{output_folder}/frame_{saved_count:04d}.jpg\", image)\n",
        "        saved_count += 1\n",
        "\n",
        "vidcap.release()\n",
        "print(f\"Extracted {saved_count} frames from {total_frames} total frames.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure torchvision is installed before loading Phi-3 Vision\n",
        "!pip install -q torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Audio extraction and transcription\n",
        "from moviepy.editor import VideoFileClip\n",
        "import whisper\n",
        "import torch\n",
        "\n",
        "# Clear GPU memory before loading large models\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Extract audio\n",
        "audio_path = \"extracted_audio.wav\"\n",
        "try:\n",
        "    video = VideoFileClip(video_path)\n",
        "    video.audio.write_audiofile(audio_path, verbose=False, logger=None)\n",
        "except Exception as e:\n",
        "    print(f\"Audio extraction failed: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Load Whisper model\n",
        "print(\"Loading Whisper model...\")\n",
        "model_whisper = whisper.load_model(\"base\")\n",
        "\n",
        "# Transcribe with progress\n",
        "print(\"Transcribing audio...\")\n",
        "result = model_whisper.transcribe(audio_path, verbose=True)\n",
        "transcript = result[\"text\"]\n",
        "print(f\"Transcription complete ({len(transcript.split())} words).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Optimized Phi-3 Vision loading\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "model_id = \"microsoft/Phi-3-vision-128k-instruct\"\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Loading Phi-3 Vision model...\")\n",
        "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\"\n",
        ").eval()\n",
        "\n",
        "def analyze_image_with_phi(image_path):\n",
        "    try:\n",
        "        image = Image.open(image_path)\n",
        "        prompt = \"<|user|>\\n<|image_1|>\\nDescribe this image in detail. Include objects, actions, and context.\\n<|end|>\\n<|assistant|>\\n\"\n",
        "        \n",
        "        inputs = processor(prompt, [image], return_tensors=\"pt\").to(model.device)\n",
        "        \n",
        "        generate_args = {\n",
        "            \"max_new_tokens\": 256,\n",
        "            \"do_sample\": False,\n",
        "            \"temperature\": 0.0\n",
        "        }\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(**inputs, **generate_args)\n",
        "            \n",
        "        response = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "        # Extract only the assistant's response\n",
        "        return response.split(\"<|assistant|>\")[-1].strip()\n",
        "    except Exception as e:\n",
        "        return f\"Description failed: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Analyze frames with progress and error handling\n",
        "import glob\n",
        "import time\n",
        "\n",
        "frame_descriptions = []\n",
        "frame_files = sorted(glob.glob(f\"{output_folder}/*.jpg\"))\n",
        "\n",
        "if not frame_files:\n",
        "    raise FileNotFoundError(\"No frames found for analysis\")\n",
        "\n",
        "print(f\"Starting analysis of {len(frame_files)} frames...\")\n",
        "\n",
        "for i, img_path in enumerate(frame_files):\n",
        "    start_time = time.time()\n",
        "    print(f\"Analyzing frame {i+1}/{len(frame_files)}: {os.path.basename(img_path)}\")\n",
        "    \n",
        "    desc = analyze_image_with_phi(img_path)\n",
        "    frame_descriptions.append(desc)\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"  Description ({elapsed:.1f}s): {desc[:80]}{'...' if len(desc)>80 else ''}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Generate comprehensive report\n",
        "from datetime import datetime\n",
        "\n",
        "report = f\"\"\"ðŸŽ¥ VIDEO ANALYSIS REPORT\n",
        "Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "Original file: {video_path}\n",
        "Duration: {video.duration:.1f} seconds\n",
        "Frames analyzed: {len(frame_descriptions)}\n",
        "\n",
        "=== KEY OBSERVATIONS ===\n",
        "\"\"\"\n",
        "\n",
        "# Add summary of first and last frames\n",
        "if frame_descriptions:\n",
        "    report += f\"- First frame: {frame_descriptions[0][:150]}...\\n\"\n",
        "    report += f\"- Last frame: {frame_descriptions[-1][:150]}...\\n\\n\"\n",
        "\n",
        "report += \"=== DETAILED FRAME DESCRIPTIONS ===\\n\"\n",
        "for i, desc in enumerate(frame_descriptions):\n",
        "    timestamp = i * 2  # 2 seconds per frame\n",
        "    mins, secs = divmod(timestamp, 60)\n",
        "    report += f\"\\n[{i+1}] @ {int(mins):02d}:{int(secs):02d}\\n{desc}\\n\"\n",
        "\n",
        "report += \"\\n\\n=== AUDIO TRANSCRIPT ===\\n\"\n",
        "report += transcript\n",
        "\n",
        "# Save report\n",
        "report_path = \"video_analysis_report.txt\"\n",
        "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"âœ… Report generated ({len(report.split())} words)! Saved to {report_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Download results\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Create zip archive\n",
        "shutil.make_archive(\"video_analysis_results\", 'zip', output_folder)\n",
        "shutil.copy(audio_path, \"transcription_audio.wav\")\n",
        "\n",
        "files.download(report_path)\n",
        "files.download(\"video_analysis_results.zip\")\n",
        "files.download(\"transcription_audio.wav\")\n",
        "\n",
        "print(\"All results downloaded.\")"
      ]
    }
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "print(\"Test cell added\")"
    ]
  }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
